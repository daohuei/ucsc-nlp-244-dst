{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e55ae-5c87-441c-ab2e-4e0cb58f9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code from PyTorch Lightning Official Website\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import random_split\n",
    "# from torchvision.datasets import MNIST\n",
    "# from torchvision import transforms\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "# class LitAutoEncoder(pl.LightningModule):\n",
    "# \tdef __init__(self):\n",
    "# \t\tsuper().__init__()\n",
    "# \t\tself.encoder = nn.Sequential(\n",
    "#       nn.Linear(28 * 28, 64),\n",
    "#       nn.ReLU(),\n",
    "#       nn.Linear(64, 3))\n",
    "# \t\tself.decoder = nn.Sequential(\n",
    "#       nn.Linear(3, 64),\n",
    "#       nn.ReLU(),\n",
    "#       nn.Linear(64, 28 * 28))\n",
    "\n",
    "# \tdef forward(self, x):\n",
    "# \t\tembedding = self.encoder(x)\n",
    "# \t\treturn embedding\n",
    "\n",
    "# \tdef configure_optimizers(self):\n",
    "# \t\toptimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "# \t\treturn optimizer\n",
    "\n",
    "# \tdef training_step(self, train_batch, batch_idx):\n",
    "# \t\tx, y = train_batch\n",
    "# \t\tx = x.view(x.size(0), -1)\n",
    "# \t\tz = self.encoder(x)    \n",
    "# \t\tx_hat = self.decoder(z)\n",
    "# \t\tloss = F.mse_loss(x_hat, x)\n",
    "# \t\tself.log('train_loss', loss)\n",
    "# \t\treturn loss\n",
    "\n",
    "# \tdef validation_step(self, val_batch, batch_idx):\n",
    "# \t\tx, y = val_batch\n",
    "# \t\tx = x.view(x.size(0), -1)\n",
    "# \t\tz = self.encoder(x)\n",
    "# \t\tx_hat = self.decoder(z)\n",
    "# \t\tloss = F.mse_loss(x_hat, x)\n",
    "# \t\tself.log('val_loss', loss)\n",
    "\n",
    "# # data\n",
    "# dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
    "# mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "# train_loader = DataLoader(mnist_train, batch_size=32)\n",
    "# val_loader = DataLoader(mnist_val, batch_size=32)\n",
    "\n",
    "# # model\n",
    "# model = LitAutoEncoder()\n",
    "\n",
    "# # training\n",
    "# trainer = pl.Trainer(gpus=4, num_nodes=8, precision=16, limit_train_batches=0.5)\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8437edf-0bc9-4c41-96f5-9d55b5eee8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cc835a1-67aa-4b42-adaf-fb5c88a50762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything, LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "AVAIL_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5280e387-7c9b-4b97-a45c-b4139db3f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUEDataModule(LightningDataModule):\n",
    "\n",
    "    task_text_field_map = {\n",
    "        \"cola\": [\"sentence\"],\n",
    "        \"sst2\": [\"sentence\"],\n",
    "        \"mrpc\": [\"sentence1\", \"sentence2\"],\n",
    "        \"qqp\": [\"question1\", \"question2\"],\n",
    "        \"stsb\": [\"sentence1\", \"sentence2\"],\n",
    "        \"mnli\": [\"premise\", \"hypothesis\"],\n",
    "        \"qnli\": [\"question\", \"sentence\"],\n",
    "        \"rte\": [\"sentence1\", \"sentence2\"],\n",
    "        \"wnli\": [\"sentence1\", \"sentence2\"],\n",
    "        \"ax\": [\"premise\", \"hypothesis\"],\n",
    "    }\n",
    "\n",
    "    glue_task_num_labels = {\n",
    "        \"cola\": 2,\n",
    "        \"sst2\": 2,\n",
    "        \"mrpc\": 2,\n",
    "        \"qqp\": 2,\n",
    "        \"stsb\": 1,\n",
    "        \"mnli\": 3,\n",
    "        \"qnli\": 2,\n",
    "        \"rte\": 2,\n",
    "        \"wnli\": 2,\n",
    "        \"ax\": 3,\n",
    "    }\n",
    "\n",
    "    loader_columns = [\n",
    "        \"datasets_idx\",\n",
    "        \"input_ids\",\n",
    "        \"token_type_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"start_positions\",\n",
    "        \"end_positions\",\n",
    "        \"labels\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        task_name: str = \"mrpc\",\n",
    "        max_seq_length: int = 128,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.task_name = task_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "\n",
    "        self.text_fields = self.task_text_field_map[task_name]\n",
    "        self.num_labels = self.glue_task_num_labels[task_name]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.dataset = datasets.load_dataset(\"glue\", self.task_name)\n",
    "        \n",
    "        # for having different mapping according to different loader\n",
    "        for split in self.dataset.keys():\n",
    "            self.dataset[split] = self.dataset[split].map(\n",
    "                self.convert_to_features,\n",
    "                batched=True,\n",
    "                remove_columns=[\"label\"],\n",
    "            )\n",
    "            self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n",
    "            self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "\n",
    "        self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # for loading dataset\n",
    "        datasets.load_dataset(\"glue\", self.task_name)\n",
    "        # for loading the tokenizer\n",
    "        AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    # the mapping function\n",
    "    def convert_to_features(self, example_batch, indices=None):\n",
    "\n",
    "        # Either encode single sentence or sentence pairs\n",
    "        if len(self.text_fields) > 1:\n",
    "            texts_or_text_pairs = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n",
    "        else:\n",
    "            texts_or_text_pairs = example_batch[self.text_fields[0]]\n",
    "\n",
    "        # Tokenize the text/text pairs\n",
    "        features = self.tokenizer.batch_encode_plus(\n",
    "            texts_or_text_pairs, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True\n",
    "        )\n",
    "\n",
    "        # Rename label to labels to make it easier to pass to model forward\n",
    "        features[\"labels\"] = example_batch[\"label\"]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e247c95e-ef84-4b89-b824-9b97cbe53e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/daohuei/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d72d952f60446a7beaa28576c4f2425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/daohuei/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5073e9e896ed4f769105936b8e7f2670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e351be6b4ac3b7f1.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7b7c1251bb41c9863eb34b429b9aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76658478796e454b8731c6515463088a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2572,  3217,  ...,     0,     0,     0],\n",
       "         [  101,  9805,  3540,  ...,     0,     0,     0],\n",
       "         [  101,  2027,  2018,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1996,  2922,  ...,     0,     0,     0],\n",
       "         [  101,  6202,  1999,  ...,     0,     0,     0],\n",
       "         [  101, 16565,  2566,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "         1, 1, 0, 0, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = GLUEDataModule(\"distilbert-base-uncased\")\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n",
    "next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523ad1b0-9a00-40d2-ab7c-9b266f883f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\n",
    "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "# batch = tok(example_english_phrase, return_tensors=\"pt\")\n",
    "# generated_ids = model.generate(batch[\"input_ids\"])\n",
    "# assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [\n",
    "#     \"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\"\n",
    "# ]\n",
    "class DSTBart(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # model_name_or_path: str,\n",
    "        num_labels: int = 0,\n",
    "        task_name: str = \"dst\",\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        eval_splits: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model_name_or_path = \"facebook/bart-base\"\n",
    "        self.config = AutoConfig.from_pretrained(self.model_name_or_path)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name_or_path, forced_bos_token_id=0)\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
    "        # self.metric = datasets.load_metric(\n",
    "        #     \"glue\", self.hparams.task_name, experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        # )\n",
    "\n",
    "    \n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "        \n",
    "        # need to embeded in post-processing code\n",
    "        if self.hparams.num_labels >= 1:\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "        elif self.hparams.num_labels == 1:\n",
    "            preds = logits.squeeze()\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # need to embeded in relative slot accuracy calculation\n",
    "        if self.hparams.task_name == \"mnli\":\n",
    "            for i, output in enumerate(outputs):\n",
    "                # matched or mismatched\n",
    "                split = self.hparams.eval_splits[i].split(\"_\")[-1]\n",
    "                # evaluation\n",
    "                preds = torch.cat([x[\"preds\"] for x in output]).detach().cpu().numpy()\n",
    "                labels = torch.cat([x[\"labels\"] for x in output]).detach().cpu().numpy()\n",
    "                loss = torch.stack([x[\"loss\"] for x in output]).mean()\n",
    "                self.log(f\"val_loss_{split}\", loss, prog_bar=True)\n",
    "                split_metrics = {\n",
    "                    f\"{k}_{split}\": v for k, v in self.metric.compute(predictions=preds, references=labels).items()\n",
    "                }\n",
    "                self.log_dict(split_metrics, prog_bar=True)\n",
    "            return loss\n",
    "\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        # ignore if in training stage\n",
    "        if stage != \"fit\":\n",
    "            return\n",
    "        # Get dataloader by calling it - train_dataloader() is called after setup() by default\n",
    "        train_loader = self.trainer.datamodule.train_dataloader()\n",
    "\n",
    "        # Calculate total steps\n",
    "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "        self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        # customize: do not do weight decay at bias weight and weights in LayerNorm Module\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "class GLUETransformer(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        num_labels: int,\n",
    "        task_name: str,\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        eval_splits: Optional[list] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n",
    "        self.metric = datasets.load_metric(\n",
    "            \"glue\", self.hparams.task_name, experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "\n",
    "        if self.hparams.num_labels >= 1:\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "        elif self.hparams.num_labels == 1:\n",
    "            preds = logits.squeeze()\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if self.hparams.task_name == \"mnli\":\n",
    "            for i, output in enumerate(outputs):\n",
    "                # matched or mismatched\n",
    "                split = self.hparams.eval_splits[i].split(\"_\")[-1]\n",
    "                preds = torch.cat([x[\"preds\"] for x in output]).detach().cpu().numpy()\n",
    "                labels = torch.cat([x[\"labels\"] for x in output]).detach().cpu().numpy()\n",
    "                loss = torch.stack([x[\"loss\"] for x in output]).mean()\n",
    "                self.log(f\"val_loss_{split}\", loss, prog_bar=True)\n",
    "                split_metrics = {\n",
    "                    f\"{k}_{split}\": v for k, v in self.metric.compute(predictions=preds, references=labels).items()\n",
    "                }\n",
    "                self.log_dict(split_metrics, prog_bar=True)\n",
    "            return loss\n",
    "\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n",
    "        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        if stage != \"fit\":\n",
    "            return\n",
    "        # Get dataloader by calling it - train_dataloader() is called after setup() by default\n",
    "        train_loader = self.trainer.datamodule.train_dataloader()\n",
    "\n",
    "        # Calculate total steps\n",
    "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
    "        self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        # customize: do not do weight decay at bias weight and weights in LayerNorm Module\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bdb3a5-44b0-454d-abf9-9e66ccb144a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Reusing dataset glue (/Users/daohuei/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9423f9f2d0945a0be2939dbb64d8676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7126e8f9bcee5c6a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82e7e24b4f24fd2a1da9623147213c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5497316eebe44c0bb956614b82aaaca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Reusing dataset glue (/Users/daohuei/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27636b25e8e24b71af264b07f5363fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/daohuei/CodeNest/ucsc-nlp-244-dst/lightning_logs\n",
      "Reusing dataset glue (/Users/daohuei/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c43d47d81a741639d4e308daaab4602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b093094f2c043049d37790dc4e47976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42caf57ae77440cb10646e47c346cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526ae1c6b9194fb2b77ab5ced65754b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2192: LightningDeprecationWarning: `Trainer.gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` or `Trainer.device_ids` to get device information instead.\n",
      "  rank_zero_deprecation(\n",
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                            | Params\n",
      "----------------------------------------------------------\n",
      "0 | model | AlbertForSequenceClassification | 11.7 M\n",
      "----------------------------------------------------------\n",
      "11.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.7 M    Total params\n",
      "46.740    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff02dd0d1be48768253b3813bc55efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "dm = GLUEDataModule(model_name_or_path=\"albert-base-v2\", task_name=\"cola\")\n",
    "dm.setup(\"fit\")\n",
    "model = GLUETransformer(\n",
    "    model_name_or_path=\"albert-base-v2\",\n",
    "    num_labels=dm.num_labels,\n",
    "    eval_splits=dm.eval_splits,\n",
    "    task_name=dm.task_name,\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=1, gpus=AVAIL_GPUS)\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9445eadb-e450-44cf-865d-07474ef89e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1c09828a0d4211a22ff9db50215931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fc2bfaa6914c7b847a5b552b5c6121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1e939d512d47b79fe2f50741821141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e975e4ccfde94d4e8414392375ab2824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /Users/daohuei/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c80cc1cfd3443f948f4eb14d9bc78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/313M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_matched split:   0%|          | 0/9796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_mismatched split:   0%|          | 0/9847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /Users/daohuei/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1eaf7751dc44d82a3ec38323677c554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a21bffe7ef24e3db6c6cde43a416c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d79ca7ae734cd6b9f73cdfb3bdfef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cb1321ab884b7c99bdd8ff67a83bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af13cc7a7c4fc0addb085ddd4ef317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db5f9d5471a4677a97ace675030dae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de71b1943334ec48e1f88753c58632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/daohuei/miniconda3/envs/py3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 1, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f4091e3c7448c1a67a2c247d309aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0             DataLoader 1\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    accuracy_matched        0.3175751268863678       0.3175751268863678\n",
      "   accuracy_mismatched       0.318856805562973        0.318856805562973\n",
      "    val_loss_matched        1.1064648628234863       1.1064648628234863\n",
      "   val_loss_mismatched      1.1063889265060425       1.1063889265060425\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_matched': 1.1064648628234863,\n",
       "  'accuracy_matched': 0.3175751268863678,\n",
       "  'val_loss_mismatched': 1.1063889265060425,\n",
       "  'accuracy_mismatched': 0.318856805562973},\n",
       " {'val_loss_matched': 1.1064648628234863,\n",
       "  'accuracy_matched': 0.3175751268863678,\n",
       "  'val_loss_mismatched': 1.1063889265060425,\n",
       "  'accuracy_mismatched': 0.318856805562973}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = GLUEDataModule(\n",
    "    model_name_or_path=\"distilbert-base-cased\",\n",
    "    task_name=\"mnli\",\n",
    ")\n",
    "dm.setup(\"fit\")\n",
    "model = GLUETransformer(\n",
    "    model_name_or_path=\"distilbert-base-cased\",\n",
    "    num_labels=dm.num_labels,\n",
    "    eval_splits=dm.eval_splits,\n",
    "    task_name=dm.task_name,\n",
    ")\n",
    "\n",
    "trainer = Trainer(gpus=AVAIL_GPUS, progress_bar_refresh_rate=20)\n",
    "trainer.validate(model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256a9f0b-543e-4d59-9895-cef120e11a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset, Split\n",
    "\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0774d540-b3d1-4189-a724-b58ff4012431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bfc7feb1c1e93a63\n",
      "Reusing dataset multi_woz_dataset (/Users/daohuei/.cache/huggingface/datasets/multi_woz_dataset/default-bfc7feb1c1e93a63/0.0.0/214b5f86ecf324bff982d075298ac2b6490bbca4895dda93d15c69bf8e8274e7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22569a88a20946a88bdc588cbeb0fad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dialogue history and target belief\n",
    "\n",
    "data_dir = Path('resources/bart/')\n",
    "\n",
    "data_files = {\n",
    "    Split.TRAIN: str((data_dir / 'train.history_belief').absolute()),\n",
    "    Split.VALIDATION: str((data_dir / 'val.history_belief').absolute()),\n",
    "    Split.TEST: str((data_dir / 'test.history_belief').absolute())\n",
    "}\n",
    "\n",
    "dataset = load_dataset('data/dataset/multiwoz_dataset.py', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "749a7d8a-12bd-4af4-bedf-818b9cf31a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/multi_woz_dataset/default-bfc7feb1c1e93a63/0.0.0/214b5f86ecf324bff982d075298ac2b6490bbca4895dda93d15c69bf8e8274e7/cache-d16f23eccc4b23c9.arrow\n",
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/multi_woz_dataset/default-bfc7feb1c1e93a63/0.0.0/214b5f86ecf324bff982d075298ac2b6490bbca4895dda93d15c69bf8e8274e7/cache-729a1d081f6da48b.arrow\n",
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/multi_woz_dataset/default-bfc7feb1c1e93a63/0.0.0/214b5f86ecf324bff982d075298ac2b6490bbca4895dda93d15c69bf8e8274e7/cache-68c0f4701c954fae.arrow\n"
     ]
    }
   ],
   "source": [
    "# flatten dialogues into turn-level, and also include the previous belief(dialogue state)\n",
    "dataset = dataset.map(flatten_conversation, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5652a986-c977-4a68-ad68-61147fabb6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/multi_woz_dataset/default-bfc7feb1c1e93a63/0.0.0/214b5f86ecf324bff982d075298ac2b6490bbca4895dda93d15c69bf8e8274e7/cache-61f9fa801989862d.arrow\n",
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/multi_woz_dataset/default-bfc7feb1c1e93a63/0.0.0/214b5f86ecf324bff982d075298ac2b6490bbca4895dda93d15c69bf8e8274e7/cache-4b08653818633b57.arrow\n",
      "Loading cached processed dataset at /Users/daohuei/.cache/huggingface/datasets/multi_woz_dataset/default-bfc7feb1c1e93a63/0.0.0/214b5f86ecf324bff982d075298ac2b6490bbca4895dda93d15c69bf8e8274e7/cache-174b0d14d0ea69dd.arrow\n"
     ]
    }
   ],
   "source": [
    "# mask specifically on the value that is having changes in the current state\n",
    "masked_deltas = dataset.map(mask_delta_beliefs, remove_columns='turn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b35a2719-fe27-48bc-a34c-9bde9c71fe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d8e7bbd54f4b0e97da66c62f2752d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56778 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8af3e7d3aa4f3a840ec4c7bc586095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7374 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64c0dce63644982a581693b606e1ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7372 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomly mask 15% of the state value\n",
    "random_masked_beliefs = dataset.map(lambda d: random_mask_beliefs(d, 0.15), remove_columns='turn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daded7d4-4627-481a-bc6b-0838b838f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148b1312ce354f5a8b7bc6fb472b5cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56778 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eadc1f2ba147eaaa2309a26f1f24a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7374 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9b38a0b0e746019a6734d5eafab5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7372 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# masked out the entity value in the utterance that should be predict as belief value\n",
    "masked_context_belief_entities = dataset.map(mask_context_belief_entities, remove_columns='turn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9476c98a-2ee4-4765-ba2b-ab235ac0c963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3461b307b2704d3e8379a2856e41e2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56778 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b3295d14884d2784c851e465e1b331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7374 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0099f09e82f140cca2dc74d30c1faa1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7372 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomly mask utterance tokens\n",
    "random_masked_utterances = dataset.map(lambda d: random_mask_utterance(d, 0.15), remove_columns='turn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebee2e-9b42-4f49-8036-eb5d2c3d2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSTDataModule(LightningDataModule):\n",
    "\n",
    "    # task_text_field_map = {\n",
    "    #     \"cola\": [\"sentence\"],\n",
    "    #     \"sst2\": [\"sentence\"],\n",
    "    #     \"mrpc\": [\"sentence1\", \"sentence2\"],\n",
    "    #     \"qqp\": [\"question1\", \"question2\"],\n",
    "    #     \"stsb\": [\"sentence1\", \"sentence2\"],\n",
    "    #     \"mnli\": [\"premise\", \"hypothesis\"],\n",
    "    #     \"qnli\": [\"question\", \"sentence\"],\n",
    "    #     \"rte\": [\"sentence1\", \"sentence2\"],\n",
    "    #     \"wnli\": [\"sentence1\", \"sentence2\"],\n",
    "    #     \"ax\": [\"premise\", \"hypothesis\"],\n",
    "    # }\n",
    "\n",
    "    # glue_task_num_labels = {\n",
    "    #     \"cola\": 2,\n",
    "    #     \"sst2\": 2,\n",
    "    #     \"mrpc\": 2,\n",
    "    #     \"qqp\": 2,\n",
    "    #     \"stsb\": 1,\n",
    "    #     \"mnli\": 3,\n",
    "    #     \"qnli\": 2,\n",
    "    #     \"rte\": 2,\n",
    "    #     \"wnli\": 2,\n",
    "    #     \"ax\": 3,\n",
    "    # }\n",
    "\n",
    "    loader_columns = [\n",
    "        # \"datasets_idx\",\n",
    "        # \"input_ids\",\n",
    "        # \"token_type_ids\",\n",
    "        # \"attention_mask\",\n",
    "        # \"start_positions\",\n",
    "        # \"end_positions\",\n",
    "        # \"labels\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        task_name: str = \"dst\",\n",
    "        max_seq_length: int = 128,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.task_name = task_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "\n",
    "        # self.text_fields = self.task_text_field_map[task_name]\n",
    "        # self.num_labels = self.glue_task_num_labels[task_name]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "#         self.dataset = datasets.load_dataset(\"glue\", self.task_name)\n",
    "        \n",
    "#         # for having different mapping according to different loader\n",
    "#         for split in self.dataset.keys():\n",
    "#             self.dataset[split] = self.dataset[split].map(\n",
    "#                 self.convert_to_features,\n",
    "#                 batched=True,\n",
    "#                 remove_columns=[\"label\"],\n",
    "#             )\n",
    "#             self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n",
    "#             self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "\n",
    "#         self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n",
    "\n",
    "        # load dialogue history and target belief\n",
    "\n",
    "        data_dir = Path('resources/bart/')\n",
    "\n",
    "        data_files = {\n",
    "            Split.TRAIN: str((data_dir / 'train.history_belief').absolute()),\n",
    "            Split.VALIDATION: str((data_dir / 'val.history_belief').absolute()),\n",
    "            Split.TEST: str((data_dir / 'test.history_belief').absolute())\n",
    "        }\n",
    "\n",
    "        self.dataset = load_dataset('data/dataset/multiwoz_dataset.py', data_files=data_files)\n",
    "\n",
    "    # def prepare_data(self):\n",
    "    #     # for loading dataset\n",
    "    #     datasets.load_dataset(\"glue\", self.task_name)\n",
    "    #     # for loading the tokenizer\n",
    "    #     AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "#     # the mapping function\n",
    "#     def convert_to_features(self, example_batch, indices=None):\n",
    "\n",
    "#         # Either encode single sentence or sentence pairs\n",
    "#         if len(self.text_fields) > 1:\n",
    "#             texts_or_text_pairs = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n",
    "#         else:\n",
    "#             texts_or_text_pairs = example_batch[self.text_fields[0]]\n",
    "\n",
    "#         # Tokenize the text/text pairs\n",
    "#         features = self.tokenizer.batch_encode_plus(\n",
    "#             texts_or_text_pairs, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True\n",
    "#         )\n",
    "\n",
    "#         # Rename label to labels to make it easier to pass to model forward\n",
    "#         features[\"labels\"] = example_batch[\"label\"]\n",
    "\n",
    "#         return features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
